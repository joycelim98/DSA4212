{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>DSA 4212: Year 2019-2020</center></h1>\n",
    "<h3><center> Assignment 1 (Deadline: 27 March 2020 at 23:59) </center></h3>\n",
    "<h3><center> To Be submitted on the lumiNUS )</center></h3>\n",
    "<h2><center> Group Number: ???? </center></h2>\n",
    "<h2><center> Group Member 1: Joyce Lim Li Jie, Student ID </center></h2>\n",
    "<h2><center> Group Member 2: Ryan Teo Jin Chuan, A0164337A </center></h2>\n",
    "<h2><center> Group Member 3: Wong Jia Hwee, Student ID </center></h2>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from jax.config import config\n",
    "config.update(\"jax_enable_x64\", True) \n",
    "\n",
    "import jax\n",
    "import jax.numpy as np\n",
    "\n",
    "import pylab as plt\n",
    "import imageio\n",
    "import os\n",
    "import numpy as onp\n",
    "from skimage.transform import rescale, resize, downscale_local_mean\n",
    "import pandas as pd\n",
    "import time\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Dowload\n",
    "1. Download the \"celeba_small.zip\" file available on lumiNUS.\n",
    "This is a 146Mo large zip-file containing 20K face images.\n",
    "2. Download the attribute file \"celeba.csv\" available on lumiNUS.\n",
    "3. Unzip the file \"celeba_small.zip\" in the directory of your choice. (Data = 175 Mo when uncompressed). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brief Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load attributes csv file\n",
    "attribute = pd.read_csv(\"celeba.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\"attribute\" is a dictionary containing several attributes for each image\n",
    "attribute.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let us list all the files in the image directory\n",
    "path = \"img_celeba_small\"\n",
    "all_img = [f for f in os.listdir(path) \n",
    "                 if os.path.isfile(os.path.join(path, f)) \n",
    "                 and f.endswith(\".jpg\")]\n",
    "nb_img = len(all_img)\n",
    "print(\"Number of images:\", nb_img)\n",
    "\n",
    "#let us keep only the relevant attributes\n",
    "attribute = attribute[:20000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let us display the first 20 images\n",
    "plt.figure(figsize=(10,2))\n",
    "for k in range(20):\n",
    "    #load image\n",
    "    im = imageio.imread(os.path.join(path, all_img[k])).astype(float)\n",
    "    #resize to 100x100 for display\n",
    "    im = resize(im, (100,100) )\n",
    "    #scale pixel intensity to [0,1] by divising by 255 and display\n",
    "    plt.subplot(2,10,k+1)\n",
    "    plt.imshow(im/255.)\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "    is_male = attribute[\"Male\"][k]\n",
    "    if is_male == 1:\n",
    "        plt.title(\"Male\")\n",
    "    else:\n",
    "        plt.title(\"Female\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let us display the first 20 images in black and white\n",
    "plt.figure(figsize=(10,2))\n",
    "for k in range(20):\n",
    "    #load image\n",
    "    im = imageio.imread(os.path.join(path, all_img[k])).astype(float)\n",
    "    #resize to 100x100\n",
    "    im = resize(im, (100,100) )\n",
    "    #transform to black and white by averaging the 3 color channels\n",
    "    im = onp.mean(im, axis=2)\n",
    "    #scale pixel intensity to [0,1] by divising by 255 and display\n",
    "    plt.subplot(2,10,k+1)\n",
    "    plt.imshow(im/255., cmap=\"gray\") # grayscale\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let us load the first 15000 images\n",
    "n_img = 15000\n",
    "img_array = onp.zeros((n_img,100,100))\n",
    "for k in range(n_img):\n",
    "    im = imageio.imread(os.path.join(path, all_img[k])).astype(float)\n",
    "    im = resize(im, (100,100))\n",
    "    im = onp.mean(im, axis=2)\n",
    "    img_array[k,:,:] = im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let us plot a few \"average\" faces\n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "plt.subplot(2,5,1)\n",
    "is_young = attribute[\"Young\"][:n_img]==1\n",
    "plt.imshow(onp.mean(img_array[is_young,:,:], axis=0), cmap=\"gray\")\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Avg(Young)\")\n",
    "\n",
    "plt.subplot(2,5,2)\n",
    "is_male = attribute[\"Male\"][:n_img]==1\n",
    "plt.imshow(onp.mean(img_array[is_male,:,:], axis=0), cmap=\"gray\")\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Avg(Male)\")\n",
    "\n",
    "\n",
    "plt.subplot(2,5,3)\n",
    "is_Goatee = attribute[\"Goatee\"][:n_img]==1\n",
    "plt.imshow(onp.mean(img_array[is_Goatee,:,:], axis=0), cmap=\"gray\")\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Avg(Goatee)\")\n",
    "\n",
    "\n",
    "plt.subplot(2,5,4)\n",
    "is_High_Cheekbones = attribute[\"High_Cheekbones\"][:n_img]==1\n",
    "plt.imshow(onp.mean(img_array[is_High_Cheekbones,:,:], axis=0), cmap=\"gray\")\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Avg(High_Cheek)\")\n",
    "\n",
    "\n",
    "plt.subplot(2,5,5)\n",
    "is_Low_Cheekbones = attribute[\"High_Cheekbones\"][:n_img]==-1\n",
    "plt.imshow(onp.mean(img_array[is_Low_Cheekbones,:,:], axis=0), cmap=\"gray\")\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Avg(Low_Cheek)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Proportion of Young people in the dataset:\", np.mean(attribute[\"Young\"]==1) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment\n",
    "\n",
    "Your assignment consists in building an algorithm that can automatically tell whether an image corresponds to a young person or not. \n",
    "\n",
    "1. You are only allowed to use a logistic regression model (possibly with regularization). \n",
    "2. You can only use the first 15,000 images to train you model. The accuracy of your model will be evaluated on the last 5,000 images.\n",
    "3. You are allowed to use whatever optimization algorithm you think is most efficient.\n",
    "4. You are allowed to do whatever pre-processing you deem appropriate.\n",
    "5. You will report the accuracy (i.e. th percentage of correctly classified) on the test dataset (i.e. the last 5,000 images). \n",
    "6. You will as well report the Area Under the Curve (AUC) of your classifier on the test dataset.\n",
    "\n",
    "### Some remarks:\n",
    "1. You can work with colored, or grayscale images.\n",
    "2. You can rescale the images to whatever resolution you think is best/efficient\n",
    "3. It is usually a good idea to rescale the pixel intensity in between 0 and 1\n",
    "4. If you fit a logistic regression in dimension $D$, with each coordinate roughly in between 0 and 1, it is usually a good idea to start with a random $\\beta$ with mean 0 (or 0.5), and standard deviation $1/\\sqrt{D}$ for each coordinate.\n",
    "5. It may be a good (or bad?) idea to increase/lower the contrast of the image. \n",
    "6. It may be a good (or bad?) idea to only consider a smaller parts of the image (eg. only the parts near the eye?) -- some parts of the image may be irrelevant.\n",
    "7. Sometimes it can be a good idea to randomly add noise, or randomly add some variations to the images. (keyword: data-augmentation)\n",
    "8. It can be a good idea to quickly explore different approaches on a subset of the dataset, and maybe at a lower resolution (to make the algorithm converge faster)\n",
    "9. The dataset is imbalanced -- there are 78% of \"Young\" people in the dataset. It is sometimes a good idea to consider a balanced subset of the dataset to fit your algorithm.\n",
    "10. Makes absolutely sure that you do not use the test dataset (i.e. the last 5,000 images) to train your algorithm.\n",
    "\n",
    "### Last Remark:\n",
    "It is absolutely crucial that your report is readable and well presented. The point of this assignment is not to show me that you know how to re-implement all the things we've done in class. The point of this assignment is to get the job done, as simply, robustly and efficiently as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Work starts here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating balanced preliminary training & test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select first 15000 rows for training dataset\n",
    "train = attribute[:15000]\n",
    "\n",
    "# create id column\n",
    "id = list(range(1,15001))\n",
    "train[\"id\"] = id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_Young = train[\"Young\"]==1\n",
    "is_Old = train[\"Young\"]==-1\n",
    "\n",
    "# subset training set by young/old\n",
    "train_y = train[is_Young] \n",
    "train_o = train[is_Old]\n",
    "\n",
    "# select indices for prelim training set\n",
    "onp.random.seed(0)\n",
    "y_pindex = onp.random.choice(len(train_y), 2000, replace=False)\n",
    "o_pindex = onp.random.choice(len(train_o), 2000, replace=False)\n",
    "\n",
    "# select prelim training data\n",
    "ptrain_y = train_y.iloc[y_pindex[:1500]] # need to test if this works\n",
    "ptest_y = train_y.iloc[y_pindex[1500:]]\n",
    "ptrain_o = train_o.iloc[o_pindex[:1500]]\n",
    "ptest_o = train_o.iloc[o_pindex[1500:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collate prelim training data (3k obs, 1.5k each class)\n",
    "ptrain = ptrain_y.append(ptrain_o)\n",
    "ptrain = ptrain.sort_values('id')\n",
    "\n",
    "# collate prelim test data (1k obs, .5k each class)\n",
    "ptest = ptest_y.append(ptest_o)\n",
    "ptest = ptest.sort_values('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of all indices selected for prelim training data\n",
    "fn_ptrain = ptrain['id'].values.tolist()\n",
    "\n",
    "# list of all indices selected for prelim test data\n",
    "fn_ptest = ptest['id'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the images from the prelim training set\n",
    "ntrain_img = 3000\n",
    "ptrain_img = onp.zeros((ntrain_img,100,100))\n",
    "for k in range(ntrain_img):\n",
    "    im = imageio.imread(os.path.join(path, all_img[fn_ptrain[k]])).astype(float)\n",
    "    im = resize(im, (100,100))\n",
    "    im = onp.mean(im, axis=2)\n",
    "    ptrain_img[k,:,:] = im\n",
    "    \n",
    "# load the images from the prelim training set\n",
    "ntest_img = 1000\n",
    "ptest_img = onp.zeros((ntest_img,100,100))\n",
    "for k in range(ntest_img):\n",
    "    im = imageio.imread(os.path.join(path, all_img[fn_ptest[k]])).astype(float)\n",
    "    im = resize(im, (100,100))\n",
    "    im = onp.mean(im, axis=2)\n",
    "    ptest_img[k,:,:] = im"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From code above, `ptrain` is the .csv equivalent for the preliminary TRAINING data while `ptrain_img` are the relevant images for the preliminary training data. `ptest` is the .csv equivalent for the preliminary TEST data while `ptest_img` are the relevant images for the preliminary training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = ptrain_img.reshape(3000,100*100)/255.\n",
    "y_train = onp.array(ptrain['Young']).astype(float)\n",
    "data_val = ptest_img.reshape(1000,100*100)/255.\n",
    "y_val = onp.array(ptest['Young']).astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimisation starts here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression loss function:\n",
    "$$ \\frac{1}{N} \\, \\sum_{i=1}^N \\log(1 + \\exp(-y_i \\, \\langle \\beta, x_i \\rangle)) $$\n",
    "\n",
    "Logistic probability:\n",
    "$$P(y=1 | x) = \\frac{1}{1 + \\exp(- \\langle \\beta, x \\rangle)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_single(beta, x):\n",
    "    \"\"\" beta is a vector of dimension 10000, and x as well \"\"\"\n",
    "    proba = 1. / (1. + np.exp(-np.dot(beta, x)))\n",
    "    return proba\n",
    "\n",
    "#make a prediction on a whole dataset\n",
    "prediction_data = jax.vmap(prediction_single, in_axes=(None,0))\n",
    "\n",
    "def loss_single(beta,x,y):\n",
    "    \"\"\"\n",
    "    beta: vector of dimension 100*100\n",
    "    x: vector of dimension 100*100\n",
    "    y: a number that equals -1 or 1\n",
    "    \"\"\"\n",
    "    return np.log(1. + np.exp(-y * np.dot(beta, x)))\n",
    "\n",
    "#compute the loss on. whole dataset\n",
    "loss_dataset = jax.vmap(loss_single, in_axes=(None,0,0))\n",
    "\n",
    "def loss(beta, data, y):\n",
    "    \"\"\" compute the mean of all the individual losses \"\"\"\n",
    "    list_of_all_losses = loss_dataset(beta, data, y)\n",
    "    return np.mean(list_of_all_losses)\n",
    "\n",
    "grad_loss = jax.jit(jax.grad(loss))\n",
    "\n",
    "def accuracy(beta, data, y):\n",
    "    pred = prediction_data(beta, data)\n",
    "    threshold = 0.5\n",
    "    prediction_binary = (pred > 0.5).astype(int)  #equals 1 if pred > threshold and 0 otherwise\n",
    "    prediction_sign = 2*(prediction_binary - 0.5) #equals 1 if pred > threshold and -1 otherwise\n",
    "    return np.mean(prediction_sign == y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = 100*100\n",
    "beta_init = onp.random.normal(0.5, scale = 1./onp.sqrt(D), size=D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter = 500\n",
    "learning_rate = 0.01\n",
    "loss_history = []\n",
    "acc_train_list = []\n",
    "acc_val_list = []\n",
    "\n",
    "\n",
    "for k in range(n_iter):\n",
    "    gradient = grad_loss(beta, data_train, y_train)\n",
    "    beta = beta - learning_rate * gradient\n",
    "    current_loss = loss(beta, data_train, y_train)\n",
    "    accuracy_train = accuracy(beta, data_train, y_train)\n",
    "    accuracy_val = accuracy(beta, data_val, y_val) \n",
    "    \n",
    "    acc_train_list.append(accuracy_train)\n",
    "    acc_val_list.append(accuracy_val)\n",
    "    if k % 50 == 0:\n",
    "        print(\"Loss:{0:.3f} \\t Accuracy: {1:.3f}% / {2:.3f}%\".format(current_loss, \\\n",
    "                                             100*accuracy_train,\\\n",
    "                                             100*accuracy_val))\n",
    "    loss_history.append(current_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L-BFGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_wrapper(beta):\n",
    "    \"\"\" only one argument \"\"\"\n",
    "    return loss(beta, data_train, y_train)\n",
    "\n",
    "grad_loss_wrapper = jax.jit(jax.grad(loss_wrapper))    \n",
    "\n",
    "def gradient_wrapper(beta):\n",
    "    \"\"\"output is a numpy array\"\"\"\n",
    "    return onp.array(grad_loss_wrapper(beta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_train_history = []  #to save the loss trajectory\n",
    "loss_val_history = []  #to save the loss trajectory\n",
    "time_history = []  #to save the compute time\n",
    "\n",
    "def save_traj(beta):\n",
    "    \"\"\" a function that saves a few statistics for later analysis\"\"\"\n",
    "    loss_train = loss(beta, data_train, y_train)\n",
    "    loss_val = loss(beta, data_val, y_val)\n",
    "    timing = time.time() - start\n",
    "    print(\"Time:{0:3f} \\t Loss(train):{1:.3f} \\t Loss(val):{2:.3f}\".format(timing, loss_train, loss_val))\n",
    "    time_history.append(timing)\n",
    "    loss_train_history.append(loss_train)\n",
    "    loss_val_history.append(loss_val)\n",
    "\n",
    "#run LBFGS\n",
    "start = time.time()\n",
    "traj = scipy.optimize.minimize(fun = loss_wrapper, #functino to minimize\n",
    "                                x0 = onp.array(beta_init), #initial guess\n",
    "                                method='L-BFGS-B',  #we want to use L-BFGS\n",
    "                                jac=gradient_wrapper, #function that computes the gradient\n",
    "                                callback=save_traj, #a function used to save some results for later pltting\n",
    "                                options={\"maxiter\":100})  #maximum number of iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent + Momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epoch = 50\n",
    "batch_size=100  #size of the mini-batch\n",
    "beta = onp.copy(beta_init)\n",
    "mov_avg_param = 0.95\n",
    "\n",
    "learning_rate = 0.1\n",
    "loss_train_history = []\n",
    "loss_val_history = []\n",
    "\n",
    "acc_train_history = []  #computed at the end of each epoch\n",
    "acc_val_history = []    #computed at the end of each epoch\n",
    "\n",
    "#initialize the gradient\n",
    "gradient = grad_loss(beta, data_train, y_train)\n",
    "\n",
    "start = time.time()\n",
    "for epoch in range(n_epoch):\n",
    "    #let us divide the learning by 2 every 10 epoch\n",
    "    if epoch % 10 == 0 and epoch >= 1:\n",
    "        learning_rate = learning_rate/2.\n",
    "        \n",
    "    for k in range(n_training_data // batch_size):  #number of batch per epoch\n",
    "\n",
    "        #first option\n",
    "        #============\n",
    "        #select at random \"batch_size\" random data point\n",
    "        #index_ = onp.random.choice(train_data_size, batch_size, replace=False)\n",
    "\n",
    "        #second option\n",
    "        #============\n",
    "        #consider the batches sequentially\n",
    "        index_ = onp.arange(k*batch_size, (k+1)*batch_size) % n_training_data\n",
    "        gradient_local = grad_loss(beta, data_train[index_], y_train[index_])\n",
    "\n",
    "        #moving average of the gradient\n",
    "        gradient = gradient*mov_avg_param + (1-mov_avg_param)*gradient_local\n",
    "\n",
    "        #gradient descent update\n",
    "        beta = beta - learning_rate * gradient\n",
    "    \n",
    "    #accuracy_train = accuracy(beta, data_train, y_train)\n",
    "    #accuracy_val = accuracy(beta, data_val, y_val) \n",
    "    #acc_train_list.append(accuracy_train)\n",
    "    #acc_val_list.append(accuracy_val)\n",
    "    \n",
    "    loss_train = loss(beta, data_train, y_train)\n",
    "    loss_val = loss(beta, data_val, y_val)\n",
    "    loss_train_history.append(loss_train)\n",
    "    loss_val_history.append(loss_val)\n",
    "    \n",
    "    timing = time.time() - start\n",
    "    print(\"epoch:{0:.0f} \\t time:{1:.0f} \\t Loss(train):{2:.3f} \\t Loss(val):{3:.3f}\".format(epoch, \n",
    "                                                                                                  timing, \n",
    "                                                                                                  loss_train, \n",
    "                                                                                                  loss_val))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
